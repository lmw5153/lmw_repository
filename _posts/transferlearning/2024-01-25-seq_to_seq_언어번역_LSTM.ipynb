{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df874ae7",
   "metadata": {},
   "source": [
    "# seq to seq LSTM model 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028a72da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3add77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import LSTM ,Dense, Bidirectional, Input, TimeDistributed\n",
    "from keras.models import Sequential ,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab012cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957883b",
   "metadata": {},
   "source": [
    "# seq2seq \n",
    "\n",
    "- input data : sequence\n",
    "- output data : sequence\n",
    "- 자연어 처리 분야에서 많이 활용\n",
    "- data에 대한 변환이 매우 중요\n",
    "- 자연어 처리를 위한 인코더와 디코더라는 모듈이 필요\n",
    "    - Encoder : 임의의 길이를 가진 문장을 고정길이 벡터로 변환하는 작업\n",
    "    - Decoder : 인코더의 수치 벡터를 통한 모델에 출력 시퀀스를 생성, 활성화함수는 softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4e263",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177319a",
   "metadata": {},
   "source": [
    "# 데이터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad6aa3",
   "metadata": {},
   "source": [
    "`-` 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "76944c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터\"\n",
    "input_texts = [\"Hello.\", \"How are you?\", \"What is your name?\", \"I'm hungry.\", \"How old are you?\"]\n",
    "target_texts = [\"안녕하세요.\", \"잘 지내니?\", \"너의 이름이 뭐니?\", \"나 배고파.\", \"너는 몇 살이니?\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b82a00",
   "metadata": {},
   "source": [
    "`-` 데이터처리, 아래의 데이터를 생성\n",
    "\n",
    "- encoder 입력데이터\n",
    "- decoder 입력데이터\n",
    "- encoder 출력데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c4e520c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 18, 23), (5, 10, 24), (5, 10, 24))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data 각 글자 집합 \n",
    "input_set = set(\" \".join(input_texts)) # 원래문장\n",
    "target_set = set(\" \".join(target_texts)) # 번역문장\n",
    "# 각 글자에 대한 숫자 부여\n",
    "input_token = dict([(char, i) for i, char in enumerate(input_set)])\n",
    "target_token = dict([(char, i) for i, char in enumerate(target_set)])\n",
    "\n",
    "# 시퀀스의 최대 길이\n",
    "max_encoder_seqlen = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seqlen = max([len(txt) for txt in target_texts])\n",
    "\n",
    "# 데이터의 중복되지 않는 총 글자수\n",
    "encoder_text_len = len(input_set)\n",
    "decoder_text_len = len(target_set)\n",
    "\n",
    "# 원핫 인코딩 zero \n",
    "encoder_inputdata= np.zeros((len(input_texts), max_encoder_seqlen, encoder_text_len ), dtype='float32') # encoder 입력데이터\n",
    "decoder_inputdata= np.zeros((len(input_texts), max_decoder_seqlen, decoder_text_len ), dtype='float32') # decoder 입력데이터\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seqlen, decoder_text_len), dtype='float32') #encoder 출력데이터\n",
    "\n",
    "encoder_inputdata.shape, decoder_inputdata.shape, decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe0b88",
   "metadata": {},
   "source": [
    "- 5개의 각각의 문장을 최대 시퀀스 길이로 확장을 시키고 해당되는 글자에 1을 부여할 것임\n",
    "- encoder와 decoder의 input data의 shape은 서로 충분히 달라질 수 있음\n",
    "- model fitting 에서 encoder, decoder 두가지 모델을 만들고 각각에 입력을 할 것임\n",
    "- 또한 encoder model에서 output은 가져오지 않고 hidden, cell state만 가지고 온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf28fe6",
   "metadata": {},
   "source": [
    "`-` 원핫인코딩(gpt도움 좀 받았습니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "00f08281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_inputdata[i, t, input_token[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_inputdata[i, t, target_token[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token[char]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9453b9",
   "metadata": {},
   "source": [
    "- decoder의 타겟 데이터는 입력데이터보다 스텝이 한칸 더 빠름\n",
    "    - seq2seq 모델의 훈련 과정을 위한 방법\n",
    "    - 미리 정답을 주어 다음 타임스탭에서 예측에활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9710a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84506509",
   "metadata": {},
   "source": [
    "# encoder\n",
    "\n",
    "- return_state=True 출력,은닉,셀 반환옵션\n",
    "- 아웃풋,히든스테이트,셀스테이트 중에 아웃풋 사용x\n",
    "- units은 인코더, 디코더 동일해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4d600c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "n= 32\n",
    "\n",
    "encoder_input = Input(shape=(None,encoder_text_len))\n",
    "\n",
    "encoder = LSTM(units=n, return_state=True) # return_state=True 출력,은닉,셀 반환옵션\n",
    "\n",
    "# 아웃풋,히든스테이트,셀스테이트 중에 아웃풋 사용x\n",
    "output, encoder_h, encoder_c = encoder(encoder_input) \n",
    "\n",
    "# decoder에서 입력할 state\n",
    "encoder_state = [encoder_h, encoder_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99567f83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da64eac",
   "metadata": {},
   "source": [
    "# decoder\n",
    "- 컨텍스트 벡터 : initial_state=encoder_state $\\rightarrow$ encoder의 정보를 decoder에게 전달\n",
    "- decoder에서는 output만을 이용해 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "71cb2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,decoder_text_len))\n",
    "\n",
    "decoder = LSTM(units=n, return_sequences=True, return_state=True)\n",
    "\n",
    "# 컨텍스트 벡터 encoder_state를 decoder로 전달\n",
    "decoder_output,decoder_h, decoder_c= decoder(decoder_input,initial_state=encoder_state) \n",
    "\n",
    "# decoder에서는 output만을 이용해 출력\n",
    "decoder_dense = Dense(units=decoder_text_len,activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7246f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4bd16708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 23)]           0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None, 24)]           0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 32),                 7168      ['input_1[0][0]']             \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32)]                                                         \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 32),           7296      ['input_2[0][0]',             \n",
      "                              (None, 32),                            'lstm[0][1]',                \n",
      "                              (None, 32)]                            'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 24)             792       ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15256 (59.59 KB)\n",
      "Trainable params: 15256 (59.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cff3e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e26174",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c7893053",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "92cd1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b985fc4c40>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_inputdata, decoder_inputdata], decoder_target_data, \n",
    "          batch_size=1, epochs=200,  verbose=0, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ecd12",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
