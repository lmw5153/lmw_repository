# seq2one 단방향 전이학습 LSTM 예제

- 전이학습에 대한 예제
- 사용할 모델은 lstm으로 아래와 같이 모든 모델을 통일하도록 함
    - seq2one 구조의 데이터
    - 단방향
    - 2층구조, 각 층의 unit은 64개
- 추가
    - GRU 모델 비교군
    - patience = 5로 설정

---

# import 


```python
from datasetsforecast.m4 import M4, M4Evaluation
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import time

from keras.layers import LSTM ,Dense ,GRU
from keras.models import Sequential
import keras.backend as K
from keras.callbacks import EarlyStopping
```

# source data

- 소스데이터는 m4데이터의 hourly data로 24주기의 계절성분을 지님
- 24주기의 타임스탭과 363436의 data size를 지닌다



```python
df,*_= M4.load(directory='data',group = 'Hourly')

lst = list(set(df.loc[:,'unique_id']))

tt = [df[df.loc[:,'unique_id']==lst[i]].iloc[:,1:4].set_index(['ds']) 
      for i in range(len(lst))]

for l in range(len(tt)):
    for s in range(1, 25):
        tt[l]['shift_{}'.format(s)] = tt[l]['y'].shift(s)
        tt[l]['shift_{}'.format(s)] = tt[l]['y'].shift(s)
        
tt=[tt[i].dropna(axis=0) for i in range(len(tt))]

train = np.concatenate([np.array(tt[i].iloc[:,1:]) for i in range(len(tt))])
y = np.concatenate([np.array(tt[i].iloc[:,0]) for i in range(len(tt))]).reshape(-1,1)

from sklearn.preprocessing import MinMaxScaler
min_max_scaler1 = MinMaxScaler()

X_scale = min_max_scaler1.fit_transform(train)
y_scale = min_max_scaler1.fit_transform(y)
```


```python
X_scale.shape, y_scale.shape
```




    ((363436, 24), (363436, 1))



---

# target data
- target data는 마지막으로부터 2400개의 정보만 쓰기로함
- target data의 크기가 커질수록 전이학습의 효력은 떨어질 것으로 예상되어짐
- 가지고 있는 data가 작을 수록 유의한 성능을 보여줄 수 있지 않을까


```python
df = pd.read_csv('C:/Users/default.DESKTOP-2ISHQBS/Documents/R/time_ele/train.csv')

arr = df.iloc[:,9] # 전력소비량
date=  pd.to_datetime(df.iloc[:,2]) # 일시

df_= pd.DataFrame({'date':date,
              'ele': arr})

df_ = df_.set_index('date')

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()

df_['mmele'] = min_max_scaler.fit_transform(df_.iloc[:].to_numpy().reshape(-1,1))

df_ = df_.drop(['ele'],axis=1)

for s in range(1, 25):
    df_['shift_{}'.format(s)] = df_['mmele'].shift(s)
    df_['shift_{}'.format(s)] = df_['mmele'].shift(s)

df_ = df_.dropna()

y = df_.iloc[:,[0]].values #scaled 

X = df_.iloc[:,1:]

from sklearn.model_selection import train_test_split

x_train,x_test, y_train , y_test = train_test_split(X.iloc[-2400:,:],y[-2400:],shuffle=False, test_size=0.1)
```


```python
x_train.shape, x_test.shape, y_train.shape, y_test.shape
```




    ((2160, 24), (240, 24), (2160, 1), (240, 1))



---

# non pretrained lstm model

- 비교군 모델
- 타겟 데이터를 직접적으로 피팅하고 예측


```python
K.clear_session()
modelnon0 = Sequential() # Sequeatial Model
modelnon0.add(LSTM(64, return_sequences=True,input_shape=(24, 1)))# (timestep, feature)
modelnon0.add(LSTM(64))
modelnon0.add(Dense(1)) # output = 1
modelnon0.compile(loss='mean_squared_error', optimizer='adam')
```


```python
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
start_timenon0 = time.time()
modelnon0.fit(x_train,y_train, epochs=100,
          batch_size=128, verbose=0, callbacks=[early_stop])
end_timenon0 = time.time()
```

    Epoch 67: early stopping
    


```python
total_training_time0 = end_timenon0 - start_timenon0

print(f"총 학습에 걸린 시간: {total_training_time0} 초")
```

    총 학습에 걸린 시간: 19.667548418045044 초
    

---

# non pretrained gru model


```python
K.clear_session()
modelnon1 = Sequential() # Sequeatial Model
modelnon1.add(GRU(64, return_sequences=True,input_shape=(24, 1)))# (timestep, feature)
modelnon1.add(GRU(64))
modelnon1.add(Dense(1)) # output = 1
modelnon1.compile(loss='mean_squared_error', optimizer='adam')
```


```python
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
start_timenon1 = time.time()
modelnon1.fit(x_train,y_train, epochs=100,
          batch_size=128, verbose=0, callbacks=[early_stop])
end_timenon1 = time.time()


total_training_time1 = end_timenon1 - start_timenon1

print(f"총 학습에 걸린 시간: {total_training_time1} 초")
```

    Epoch 38: early stopping
    총 학습에 걸린 시간: 11.112077236175537 초
    

---

---

# pretrained LSTM model01
- 2개의 레이어를 전이학습에 사용
- 따라서 모든 시퀀스를 반환해야하기 때문에 return_sequences=True 사용
- output 레이어는 사용하지 않음


```python
K.clear_session()
model01 = Sequential() # Sequeatial Model
model01.add(LSTM(64, return_sequences=True, input_shape=(24, 1))) # (timestep, feature)
model01.add(LSTM(64, return_sequences=True)) # 연결한 모델의 차원을 맞추기 위해 시퀀스 반환을 함
model01.add(Dense(1)) # output 사용 x
model01.compile(loss='mean_squared_error', optimizer='adam')
```


```python
np.random.seed(1)
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)

model01.fit(X_scale, y_scale, epochs=100,
          batch_size=128, verbose=1, callbacks=[early_stop])
```

    Epoch 1/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 1.5510e-04
    Epoch 2/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 2.3086e-05
    Epoch 3/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 9.9663e-06
    Epoch 4/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 7.1897e-06
    Epoch 5/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 5.9788e-06
    Epoch 6/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 5.9358e-06
    Epoch 7/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 4.2987e-06
    Epoch 8/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 3.5514e-06
    Epoch 9/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 3.5268e-06
    Epoch 10/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 2.9597e-06
    Epoch 11/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 2.8063e-06
    Epoch 12/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 2.6379e-06
    Epoch 13/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 2.7837e-06
    Epoch 14/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 2.1927e-06
    Epoch 15/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 2.0995e-06
    Epoch 16/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.8277e-06
    Epoch 17/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.8678e-06
    Epoch 18/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.8333e-06
    Epoch 19/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.7968e-06
    Epoch 20/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.7627e-06
    Epoch 21/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.7384e-06
    Epoch 22/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.5855e-06
    Epoch 23/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.3717e-06
    Epoch 24/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.5486e-06
    Epoch 25/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 1.5071e-06
    Epoch 26/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 1.2526e-06
    Epoch 27/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 1.3948e-06
    Epoch 28/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 1.3545e-06
    Epoch 29/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 1.3352e-06
    Epoch 30/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.1178e-06
    Epoch 31/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.1552e-06
    Epoch 32/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 1.2054e-06
    Epoch 33/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 1.0987e-06
    Epoch 34/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.1870e-06
    Epoch 35/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.1414e-06
    Epoch 36/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 1.0613e-06
    Epoch 37/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.1556e-06
    Epoch 38/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.0281e-06
    Epoch 39/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.2468e-06
    Epoch 40/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.0197e-06
    Epoch 41/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 1.0237e-06
    Epoch 42/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 9.2948e-07
    Epoch 43/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 1.0933e-06
    Epoch 44/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 9.9432e-07
    Epoch 45/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 9.8786e-07
    Epoch 46/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 9.8600e-07
    Epoch 47/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 9.4820e-07
    Epoch 47: early stopping
    




    <keras.src.callbacks.History at 0x293e1304d60>



---

# pretrained GRU model02


```python
K.clear_session()
model02 = Sequential() # Sequeatial Model
model02.add(GRU(64, return_sequences=True, input_shape=(24, 1))) # (timestep, feature)
model02.add(GRU(64, return_sequences=True)) # 연결한 모델의 차원을 맞추기 위해 시퀀스 반환을 함
model02.add(Dense(1)) # output 사용 x
model02.compile(loss='mean_squared_error', optimizer='adam')
```


```python
np.random.seed(1)
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)

model02.fit(X_scale, y_scale, epochs=100,
          batch_size=128, verbose=1, callbacks=[early_stop])
```

    Epoch 1/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 1.3124e-04
    Epoch 2/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.8108e-05
    Epoch 3/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 5.9503e-06
    Epoch 4/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 4.5513e-06
    Epoch 5/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 4.1325e-06
    Epoch 6/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 3.2977e-06
    Epoch 7/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 2.6136e-06
    Epoch 8/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 2.8200e-06
    Epoch 9/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 2.1729e-06
    Epoch 10/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 2.0409e-06
    Epoch 11/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.8211e-06
    Epoch 12/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.9741e-06
    Epoch 13/100
    2840/2840 [==============================] - 50s 17ms/step - loss: 1.8202e-06
    Epoch 14/100
    2840/2840 [==============================] - 50s 17ms/step - loss: 1.6006e-06
    Epoch 15/100
    2840/2840 [==============================] - 50s 18ms/step - loss: 1.4930e-06
    Epoch 16/100
    2840/2840 [==============================] - 50s 17ms/step - loss: 1.6434e-06
    Epoch 17/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.4572e-06
    Epoch 18/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.4490e-06
    Epoch 19/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.2604e-06
    Epoch 20/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 1.3263e-06
    Epoch 21/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 1.2169e-06
    Epoch 22/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.3475e-06
    Epoch 23/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.1541e-06
    Epoch 24/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.3074e-06
    Epoch 25/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.1641e-06
    Epoch 26/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.1794e-06
    Epoch 27/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 1.3065e-06
    Epoch 28/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 1.1124e-06
    Epoch 29/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.0759e-06
    Epoch 30/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.0635e-06
    Epoch 31/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 1.1038e-06
    Epoch 32/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 1.0602e-06
    Epoch 33/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.0517e-06
    Epoch 34/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 9.5828e-07
    Epoch 35/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 9.8594e-07
    Epoch 36/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 9.5663e-07
    Epoch 37/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 9.6858e-07
    Epoch 38/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 1.0474e-06
    Epoch 39/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 9.2074e-07
    Epoch 40/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 8.8798e-07
    Epoch 41/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 1.0004e-06
    Epoch 42/100
    2840/2840 [==============================] - 48s 17ms/step - loss: 9.0615e-07
    Epoch 43/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 8.9764e-07
    Epoch 44/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 8.4829e-07
    Epoch 45/100
    2840/2840 [==============================] - 47s 17ms/step - loss: 9.0086e-07
    Epoch 46/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 1.0873e-06
    Epoch 47/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 8.6702e-07
    Epoch 48/100
    2840/2840 [==============================] - 47s 16ms/step - loss: 9.1262e-07
    Epoch 49/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 8.3843e-07
    Epoch 50/100
    2840/2840 [==============================] - 49s 17ms/step - loss: 8.5161e-07
    Epoch 51/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 9.2233e-07
    Epoch 52/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 8.0295e-07
    Epoch 53/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 8.7102e-07
    Epoch 54/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 8.7360e-07
    Epoch 55/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 9.1535e-07
    Epoch 56/100
    2840/2840 [==============================] - 45s 16ms/step - loss: 8.4392e-07
    Epoch 57/100
    2840/2840 [==============================] - 46s 16ms/step - loss: 8.4165e-07
    Epoch 57: early stopping
    




    <keras.src.callbacks.History at 0x293edff2110>



---

# transfer learning freezing lstm model011



```python
#from tensorflow.keras import layers
```


```python
for layer in model01.layers:
    layer.trainable = False # freezing
    
pretrained_layers = model01.layers[:-1]
model011 = Sequential(pretrained_layers)
model011.add(LSTM(64, return_sequences=False ,input_shape =(24,64)))
model011.add(Dense(1))
model011.compile(loss='mean_squared_error', optimizer='adam')
```

`-` fitting


```python
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=0)
start_time21 = time.time()
model011.fit(x_train, y_train, epochs=100,
          batch_size=128, verbose=0, callbacks=[early_stop])
end_time21 = time.time()
```


```python
total_training_time011 = end_time21 - start_time21

print(f"총 학습에 걸린 시간: {total_training_time011} 초")
```

    총 학습에 걸린 시간: 7.003185510635376 초
    

---

# transfer learning no freezing lstm model012


```python
pretrained_layers = model01.layers[:-1]
for layer in model01.layers:
    layer.trainable = True # freezing
    

model012 = Sequential(pretrained_layers)
model012.add(LSTM(64, input_shape=(24,64)))
model012.add(Dense(1))
model012.compile(loss='mean_squared_error', optimizer='adam')
```

`-` fitting


```python
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=0)
start_time22 = time.time()
model012.fit(x_train, y_train, epochs=100,
          batch_size=128, verbose=0, callbacks=[early_stop])
end_time22 = time.time()
```


```python
total_training_time012 = end_time22 - start_time22

print(f"총 학습에 걸린 시간: {total_training_time012} 초")
```

    총 학습에 걸린 시간: 19.998873949050903 초
    

---

# transfer learning freezing GRU model021


```python
pretrained_layers = model02.layers[:-1]
for layer in model02.layers:
    layer.trainable = False # freezing
    

model021 = Sequential(pretrained_layers)
model021.add(GRU(64, input_shape=(24,64)))
model021.add(Dense(1))
model021.compile(loss='mean_squared_error', optimizer='adam')
```


```python
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
start_time021 = time.time()
model021.fit(x_train, y_train, epochs=100,
          batch_size=128, verbose=0, callbacks=[early_stop])
end_time021 = time.time()

total_training_time021 = end_time021 - start_time021

print(f"총 학습에 걸린 시간: {total_training_time021} 초")
```

    Epoch 30: early stopping
    총 학습에 걸린 시간: 9.26909327507019 초
    

---

# transfer learning freezing GRU model022


```python
pretrained_layers = model02.layers[:-1]
for layer in model02.layers:
    layer.trainable = True # freezing
    

model022 = Sequential(pretrained_layers)
model022.add(GRU(64, input_shape=(24,64)))
model022.add(Dense(1))
model022.compile(loss='mean_squared_error', optimizer='adam')
```


```python
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
start_time022 = time.time()
model022.fit(x_train, y_train, epochs=100,
          batch_size=128, verbose=0, callbacks=[early_stop])
end_time022 = time.time()

total_training_time022 = end_time022 - start_time022

print(f"총 학습에 걸린 시간: {total_training_time022} 초")
```

    Epoch 31: early stopping
    총 학습에 걸린 시간: 14.62380576133728 초
    

---

# test


```python
pred01 = modelnon0.predict(x_test)
pred02 = modelnon1.predict(x_test)
pred011 = model011.predict(x_test)
pred012 = model012.predict(x_test)
pred021 = model021.predict(x_test)
pred022 = model022.predict(x_test)
```

    8/8 [==============================] - 0s 4ms/step
    8/8 [==============================] - 0s 3ms/step
    8/8 [==============================] - 0s 5ms/step
    8/8 [==============================] - 0s 5ms/step
    8/8 [==============================] - 1s 4ms/step
    8/8 [==============================] - 1s 4ms/step
    

`-` lstm 그래프


```python
n =y_test.shape[0]
plt.plot(pred01[:n],label = 'LSTM_not_pretrain',color='skyblue')
plt.plot(pred011[:n],label = 'LSTM_all_freezing',color='orange')
plt.plot(pred022[:n],label = 'LSTM_not_freezing',color = 'red')
plt.plot(y_test.flatten()[:n],label = 'observed',color = 'black')
plt.legend(loc='lower left')
plt.show()
```


    
![png](output_54_0.png)
    


`-` gru 그래프


```python
n =y_test.shape[0]
plt.plot(pred02[:n],label = 'GRU_not_pretrain',color='skyblue')
plt.plot(pred021[:n],label = 'GRU_not_freezing',color='orange')
plt.plot(pred022[:n],label = 'GRU_not_freezing',color='red')
plt.plot(y_test.flatten()[:n],label = 'observed',color = 'black')
plt.legend(loc='lower left')
plt.show()
```


    
![png](output_56_0.png)
    


`-` 평가지표


```python
def MSE(y_pred, y_true):
    return np.mean((y_true - y_pred)**2)


def MAE(y_pred, y_true):
    absolute_errors = np.abs(y_true - y_pred)
    mae = np.mean(absolute_errors)
    return mae

def MAPE(y_true, y_pred):
    epsilon = 1e-10
    percentage_errors = np.abs((y_true - y_pred) / (y_true + epsilon)) * 100
    percentage_errors = np.nan_to_num(percentage_errors)
    mape = np.mean(percentage_errors)  
    return mape
```


```python
predlst= [pred01,pred02,pred011,pred012,pred021,pred022]
y_observed= y_test.flatten()
totaltime = total_training_time0,total_training_time1,total_training_time011,total_training_time012,total_training_time021,total_training_time022
name = ['modelnon01','modelnon02','model011','model012','model021','model022']
mse= [MSE(predlst[i],y_observed) for i in range(len(predlst))]
mae= [MAE(predlst[i],y_observed) for i in range(len(predlst))]
mape =[MAPE(predlst[i],y_observed) for i in range(len(predlst))]
```


```python
pd.options.display.float_format = '{:.7f}'.format
pd.DataFrame({'time(second)':totaltime,
             'MSE':mse,
              'MAE':mae,
             'MAPE':mape},index=name)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>time(second)</th>
      <th>MSE</th>
      <th>MAE</th>
      <th>MAPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>modelnon01</th>
      <td>19.6675484</td>
      <td>0.0003239</td>
      <td>0.0144671</td>
      <td>51.6373777</td>
    </tr>
    <tr>
      <th>modelnon02</th>
      <td>11.1120772</td>
      <td>0.0003038</td>
      <td>0.0139487</td>
      <td>59.5850496</td>
    </tr>
    <tr>
      <th>model011</th>
      <td>7.0031855</td>
      <td>0.0006325</td>
      <td>0.0211886</td>
      <td>1884.7434892</td>
    </tr>
    <tr>
      <th>model012</th>
      <td>19.9988739</td>
      <td>0.0003006</td>
      <td>0.0138186</td>
      <td>50.8377988</td>
    </tr>
    <tr>
      <th>model021</th>
      <td>9.2690933</td>
      <td>0.0006449</td>
      <td>0.0213135</td>
      <td>1294.4216252</td>
    </tr>
    <tr>
      <th>model022</th>
      <td>14.6238058</td>
      <td>0.0003101</td>
      <td>0.0140691</td>
      <td>48.7499777</td>
    </tr>
  </tbody>
</table>
</div>


